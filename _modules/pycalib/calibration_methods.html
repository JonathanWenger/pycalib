

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>pycalib.calibration_methods &mdash; pycalib  documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/graphviz.css" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home"> pycalib
          

          
          </a>

          
            
            
              <div class="version">
                0.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../intro.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../modules.html">Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../examples.html">Examples</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">pycalib</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../index.html">Module code</a> &raquo;</li>
        
      <li>pycalib.calibration_methods</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for pycalib.calibration_methods</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;Calibration Methods.&quot;&quot;&quot;</span>

<span class="c1"># Standard library imports</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">numpy.matlib</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># SciPy imports</span>
<span class="kn">import</span> <span class="nn">scipy.stats</span>
<span class="kn">import</span> <span class="nn">scipy.optimize</span>
<span class="kn">import</span> <span class="nn">scipy.special</span>
<span class="kn">import</span> <span class="nn">scipy.cluster.vq</span>

<span class="c1"># Scikit learn imports</span>
<span class="kn">import</span> <span class="nn">sklearn</span>
<span class="kn">import</span> <span class="nn">sklearn.multiclass</span>
<span class="kn">import</span> <span class="nn">sklearn.utils</span>
<span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="kn">import</span> <span class="n">clone</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelBinarizer</span>
<span class="kn">from</span> <span class="nn">sklearn.utils.validation</span> <span class="kn">import</span> <span class="n">check_is_fitted</span>
<span class="kn">from</span> <span class="nn">sklearn.utils._joblib</span> <span class="kn">import</span> <span class="n">Parallel</span>
<span class="kn">from</span> <span class="nn">sklearn.utils._joblib</span> <span class="kn">import</span> <span class="n">delayed</span>

<span class="c1"># Calibration models</span>
<span class="kn">import</span> <span class="nn">sklearn.isotonic</span>
<span class="kn">import</span> <span class="nn">sklearn.linear_model</span>
<span class="kn">import</span> <span class="nn">betacal</span>

<span class="c1"># Gaussian Processes and TensorFlow</span>
<span class="kn">import</span> <span class="nn">gpflow</span>
<span class="kn">from</span> <span class="nn">pycalib</span> <span class="kn">import</span> <span class="n">gp_classes</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Turn off tensorflow deprecation warnings</span>
<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="kn">import</span> <span class="n">module_wrapper</span> <span class="k">as</span> <span class="n">deprecation</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="kn">import</span> <span class="n">deprecation_wrapper</span> <span class="k">as</span> <span class="n">deprecation</span>
<span class="n">deprecation</span><span class="o">.</span><span class="n">_PER_MODULE_WARNING_LIMIT</span> <span class="o">=</span> <span class="mi">0</span>

<span class="c1"># Plotting</span>
<span class="kn">import</span> <span class="nn">pycalib.texfig</span>

<span class="c1"># Ignore binned_statistic FutureWarning</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">simplefilter</span><span class="p">(</span><span class="n">action</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="ne">FutureWarning</span><span class="p">)</span>


<div class="viewcode-block" id="CalibrationMethod"><a class="viewcode-back" href="../../automod/pycalib.calibration_methods.CalibrationMethod.html#pycalib.calibration_methods.CalibrationMethod">[docs]</a><span class="k">class</span> <span class="nc">CalibrationMethod</span><span class="p">(</span><span class="n">sklearn</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">BaseEstimator</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A generic class for probability calibration</span>

<span class="sd">    A calibration method takes a set of posterior class probabilities and transform them into calibrated posterior</span>
<span class="sd">    probabilities. Calibrated in this sense means that the empirical frequency of a correct class prediction matches its</span>
<span class="sd">    predicted posterior probability.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

<div class="viewcode-block" id="CalibrationMethod.fit"><a class="viewcode-back" href="../../automod/pycalib.calibration_methods.CalibrationMethod.html#pycalib.calibration_methods.CalibrationMethod.fit">[docs]</a>    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fit the calibration method based on the given uncalibrated class probabilities X and ground truth labels y.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like, shape (n_samples, n_classes)</span>
<span class="sd">            Training data, i.e. predicted probabilities of the base classifier on the calibration set.</span>
<span class="sd">        y : array-like, shape (n_samples,)</span>
<span class="sd">            Target classes.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self : object</span>
<span class="sd">            Returns an instance of self.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Subclass must implement this method.&quot;</span><span class="p">)</span></div>

<div class="viewcode-block" id="CalibrationMethod.predict_proba"><a class="viewcode-back" href="../../automod/pycalib.calibration_methods.CalibrationMethod.html#pycalib.calibration_methods.CalibrationMethod.predict_proba">[docs]</a>    <span class="k">def</span> <span class="nf">predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute calibrated posterior probabilities for a given array of posterior probabilities from an arbitrary</span>
<span class="sd">        classifier.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like, shape (n_samples, n_classes)</span>
<span class="sd">            The uncalibrated posterior probabilities.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        P : array, shape (n_samples, n_classes)</span>
<span class="sd">            The predicted probabilities.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Subclass must implement this method.&quot;</span><span class="p">)</span></div>

<div class="viewcode-block" id="CalibrationMethod.predict"><a class="viewcode-back" href="../../automod/pycalib.calibration_methods.CalibrationMethod.html#pycalib.calibration_methods.CalibrationMethod.predict">[docs]</a>    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Predict the class of new samples after scaling. Predictions are identical to the ones from the uncalibrated</span>
<span class="sd">        classifier.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like, shape (n_samples, n_classes)</span>
<span class="sd">            The uncalibrated posterior probabilities.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        C : array, shape (n_samples,)</span>
<span class="sd">            The predicted classes.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span></div>

<div class="viewcode-block" id="CalibrationMethod.plot"><a class="viewcode-back" href="../../automod/pycalib.calibration_methods.CalibrationMethod.html#pycalib.calibration_methods.CalibrationMethod.plot">[docs]</a>    <span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">filename</span><span class="p">,</span> <span class="n">xlim</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Plot the calibration map.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        xlim : array-like</span>
<span class="sd">            Range of inputs of the calibration map to be plotted.</span>

<span class="sd">        **kwargs :</span>
<span class="sd">            Additional arguments passed on to :func:`matplotlib.plot`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># TODO: Fix this plotting function</span>

        <span class="c1"># Generate data and transform</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10000</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span><span class="mi">1</span> <span class="o">-</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">]))[:,</span> <span class="mi">1</span><span class="p">]</span>

        <span class="c1"># Plot and label</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">xlim</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;p(y=1|x)&quot;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;f(p(y=1|x))&quot;</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="NoCalibration"><a class="viewcode-back" href="../../automod/pycalib.calibration_methods.NoCalibration.html#pycalib.calibration_methods.NoCalibration">[docs]</a><span class="k">class</span> <span class="nc">NoCalibration</span><span class="p">(</span><span class="n">CalibrationMethod</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A class that performs no calibration.</span>

<span class="sd">    This class can be used as a baseline for benchmarking.</span>

<span class="sd">    logits : bool, default=False</span>
<span class="sd">        Are the inputs for calibration logits (e.g. from a neural network)?</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span>

<div class="viewcode-block" id="NoCalibration.fit"><a class="viewcode-back" href="../../automod/pycalib.calibration_methods.NoCalibration.html#pycalib.calibration_methods.NoCalibration.fit">[docs]</a>    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="NoCalibration.predict_proba"><a class="viewcode-back" href="../../automod/pycalib.calibration_methods.NoCalibration.html#pycalib.calibration_methods.NoCalibration.predict_proba">[docs]</a>    <span class="k">def</span> <span class="nf">predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">logits</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">scipy</span><span class="o">.</span><span class="n">special</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">X</span></div></div>


<div class="viewcode-block" id="TemperatureScaling"><a class="viewcode-back" href="../../automod/pycalib.calibration_methods.TemperatureScaling.html#pycalib.calibration_methods.TemperatureScaling">[docs]</a><span class="k">class</span> <span class="nc">TemperatureScaling</span><span class="p">(</span><span class="n">CalibrationMethod</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Probability calibration using temperature scaling</span>

<span class="sd">    Temperature scaling [1]_ is a one parameter multi-class scaling method. Output confidence scores are calibrated,</span>
<span class="sd">    meaning they match empirical frequencies of the associated class prediction. Temperature scaling does not change the</span>
<span class="sd">    class predictions of the underlying model.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    T_init : float</span>
<span class="sd">        Initial temperature parameter used for scaling. This parameter is optimized in order to calibrate output</span>
<span class="sd">        probabilities.</span>
<span class="sd">    verbose : bool</span>
<span class="sd">        Print information on optimization procedure.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] On calibration of modern neural networks, C. Guo, G. Pleiss, Y. Sun, K. Weinberger, ICML 2017</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">T_init</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">T_init</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Temperature not greater than 0.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">T_init</span> <span class="o">=</span> <span class="n">T_init</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>

<div class="viewcode-block" id="TemperatureScaling.fit"><a class="viewcode-back" href="../../automod/pycalib.calibration_methods.TemperatureScaling.html#pycalib.calibration_methods.TemperatureScaling.fit">[docs]</a>    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fit the calibration method based on the given uncalibrated class probabilities or logits X and ground truth</span>
<span class="sd">        labels y.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like, shape (n_samples, n_classes)</span>
<span class="sd">            Training data, i.e. predicted probabilities or logits of the base classifier on the calibration set.</span>
<span class="sd">        y : array-like, shape (n_samples,)</span>
<span class="sd">            Target classes.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self : object</span>
<span class="sd">            Returns an instance of self.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Define objective function (NLL / cross entropy)</span>
        <span class="k">def</span> <span class="nf">objective</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>
            <span class="c1"># Calibrate with given T</span>
            <span class="n">P</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">special</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">X</span> <span class="o">/</span> <span class="n">T</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

            <span class="c1"># Compute negative log-likelihood</span>
            <span class="n">P_y</span> <span class="o">=</span> <span class="n">P</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])),</span> <span class="n">y</span><span class="p">]</span>
            <span class="n">tiny</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float</span><span class="p">)</span><span class="o">.</span><span class="n">tiny</span>  <span class="c1"># to avoid division by 0 warning</span>
            <span class="n">NLL</span> <span class="o">=</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">P_y</span> <span class="o">+</span> <span class="n">tiny</span><span class="p">))</span>
            <span class="k">return</span> <span class="n">NLL</span>

        <span class="c1"># Derivative of the objective with respect to the temperature T</span>
        <span class="k">def</span> <span class="nf">gradient</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>
            <span class="c1"># Exponential terms</span>
            <span class="n">E</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">X</span> <span class="o">/</span> <span class="n">T</span><span class="p">)</span>

            <span class="c1"># Gradient</span>
            <span class="n">dT_i</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">E</span> <span class="o">*</span> <span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="n">X</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])),</span> <span class="n">y</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span> \
                   <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">E</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">grad</span> <span class="o">=</span> <span class="o">-</span> <span class="n">dT_i</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">T</span> <span class="o">**</span> <span class="mi">2</span>
            <span class="k">return</span> <span class="n">grad</span>

        <span class="c1"># Optimize</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">T</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">optimize</span><span class="o">.</span><span class="n">fmin_bfgs</span><span class="p">(</span><span class="n">f</span><span class="o">=</span><span class="n">objective</span><span class="p">,</span> <span class="n">x0</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">T_init</span><span class="p">,</span>
                                          <span class="n">fprime</span><span class="o">=</span><span class="n">gradient</span><span class="p">,</span> <span class="n">gtol</span><span class="o">=</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">disp</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># Check for T &gt; 0</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">T</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Temperature not greater than 0.&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="TemperatureScaling.predict_proba"><a class="viewcode-back" href="../../automod/pycalib.calibration_methods.TemperatureScaling.html#pycalib.calibration_methods.TemperatureScaling.predict_proba">[docs]</a>    <span class="k">def</span> <span class="nf">predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute calibrated posterior probabilities for a given array of posterior probabilities from an arbitrary</span>
<span class="sd">        classifier.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like, shape (n_samples, n_classes)</span>
<span class="sd">            The uncalibrated posterior probabilities.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        P : array, shape (n_samples, n_classes)</span>
<span class="sd">            The predicted probabilities.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Check is fitted</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;T&quot;</span><span class="p">)</span>

        <span class="c1"># Transform with scaled softmax</span>
        <span class="k">return</span> <span class="n">scipy</span><span class="o">.</span><span class="n">special</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">X</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span></div>

<div class="viewcode-block" id="TemperatureScaling.latent"><a class="viewcode-back" href="../../automod/pycalib.calibration_methods.TemperatureScaling.html#pycalib.calibration_methods.TemperatureScaling.latent">[docs]</a>    <span class="k">def</span> <span class="nf">latent</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Evaluate the latent function Tz of temperature scaling.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        z : array-like, shape=(n_evaluations,)</span>
<span class="sd">            Input confidence for which to evaluate the latent function.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        f : array-like, shape=(n_evaluations,)</span>
<span class="sd">            Values of the latent function at z.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;T&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">T</span> <span class="o">*</span> <span class="n">z</span></div>

<div class="viewcode-block" id="TemperatureScaling.plot_latent"><a class="viewcode-back" href="../../automod/pycalib.calibration_methods.TemperatureScaling.html#pycalib.calibration_methods.TemperatureScaling.plot_latent">[docs]</a>    <span class="k">def</span> <span class="nf">plot_latent</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">filename</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Plot the latent function of the calibration method.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        z : array-like, shape=(n_evaluations,)</span>
<span class="sd">            Input confidence to plot latent function for.</span>
<span class="sd">        filename :</span>
<span class="sd">            Filename / -path where to save output.</span>
<span class="sd">        kwargs</span>
<span class="sd">            Additional arguments passed on to matplotlib.pyplot.subplots.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;T&quot;</span><span class="p">)</span>

        <span class="c1"># Plot latent function</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">pycalib</span><span class="o">.</span><span class="n">texfig</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">axes</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">T</span> <span class="o">*</span> <span class="n">z</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;latent function&quot;</span><span class="p">)</span>
        <span class="n">axes</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;$T</span><span class="se">\\</span><span class="s2">bm</span><span class="si">{z}</span><span class="s2">$&quot;</span><span class="p">)</span>
        <span class="n">axes</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;$</span><span class="se">\\</span><span class="s2">bm</span><span class="si">{z}</span><span class="s2">_k$&quot;</span><span class="p">)</span>
        <span class="n">fig</span><span class="o">.</span><span class="n">align_labels</span><span class="p">()</span>

        <span class="c1"># Save plot to file</span>
        <span class="n">pycalib</span><span class="o">.</span><span class="n">texfig</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span></div></div>


<div class="viewcode-block" id="PlattScaling"><a class="viewcode-back" href="../../automod/pycalib.calibration_methods.PlattScaling.html#pycalib.calibration_methods.PlattScaling">[docs]</a><span class="k">class</span> <span class="nc">PlattScaling</span><span class="p">(</span><span class="n">CalibrationMethod</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Probability calibration using Platt scaling</span>

<span class="sd">    Platt scaling [1]_ [2]_ is a parametric method designed to output calibrated posterior probabilities for (non-probabilistic)</span>
<span class="sd">    binary classifiers. It was originally introduced in the context of SVMs. It works by fitting a logistic</span>
<span class="sd">    regression model to the model output using the negative log-likelihood as a loss function.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    regularization : float, default=10^(-12)</span>
<span class="sd">        Regularization constant, determining degree of regularization in logistic regression.</span>
<span class="sd">    random_state : int, RandomState instance or None, optional (default=None)</span>
<span class="sd">        The seed of the pseudo random number generator to use when shuffling the data.</span>
<span class="sd">        If `int`, `random_state` is the seed used by the random number generator;</span>
<span class="sd">        If `RandomState` instance, `random_state` is the random number generator;</span>
<span class="sd">        If `None`, the random number generator is the RandomState instance used</span>
<span class="sd">        by `np.random`.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] Platt, J. C. Probabilistic Outputs for Support Vector Machines and Comparisons to Regularized Likelihood</span>
<span class="sd">           Methods in Advances in Large-Margin Classifiers (MIT Press, 1999)</span>
<span class="sd">    .. [2] Lin, H.-T., Lin, C.-J. &amp; Weng, R. C. A note on Platt’s probabilistic outputs for support vector machines.</span>
<span class="sd">           Machine learning 68, 267–276 (2007)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">regularization</span><span class="o">=</span><span class="mi">10</span> <span class="o">**</span> <span class="o">-</span><span class="mi">12</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">regularization</span> <span class="o">=</span> <span class="n">regularization</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">check_random_state</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>

<div class="viewcode-block" id="PlattScaling.fit"><a class="viewcode-back" href="../../automod/pycalib.calibration_methods.PlattScaling.html#pycalib.calibration_methods.PlattScaling.fit">[docs]</a>    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fit the calibration method based on the given uncalibrated class probabilities X and ground truth labels y.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like, shape (n_samples, n_classes)</span>
<span class="sd">            Training data, i.e. predicted probabilities of the base classifier on the calibration set.</span>
<span class="sd">        y : array-like, shape (n_samples,)</span>
<span class="sd">            Target classes.</span>
<span class="sd">        n_jobs : int or None, optional (default=None)</span>
<span class="sd">            The number of jobs to use for the computation.</span>
<span class="sd">            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.</span>
<span class="sd">            ``-1`` means using all processors.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self : object</span>
<span class="sd">            Returns an instance of self.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Calibration training data must have shape (n_samples, n_classes).&quot;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">X</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logistic_regressor_</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">linear_model</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">1</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">regularization</span><span class="p">,</span>
                                                                               <span class="n">solver</span><span class="o">=</span><span class="s1">&#39;lbfgs&#39;</span><span class="p">,</span>
                                                                               <span class="n">random_state</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logistic_regressor_</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">X</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">onevsrest_calibrator_</span> <span class="o">=</span> <span class="n">OneVsRestCalibrator</span><span class="p">(</span><span class="n">calibrator</span><span class="o">=</span><span class="n">clone</span><span class="p">(</span><span class="bp">self</span><span class="p">),</span> <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">onevsrest_calibrator_</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="PlattScaling.predict_proba"><a class="viewcode-back" href="../../automod/pycalib.calibration_methods.PlattScaling.html#pycalib.calibration_methods.PlattScaling.predict_proba">[docs]</a>    <span class="k">def</span> <span class="nf">predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute calibrated posterior probabilities for a given array of posterior probabilities from an arbitrary</span>
<span class="sd">        classifier.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like, shape (n_samples, n_classes)</span>
<span class="sd">            The uncalibrated posterior probabilities.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        P : array, shape (n_samples, n_classes)</span>
<span class="sd">            The predicted probabilities.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Calibration data must have shape (n_samples, n_classes).&quot;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">X</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;logistic_regressor_&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">logistic_regressor_</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="k">elif</span> <span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">X</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;onevsrest_calibrator_&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">onevsrest_calibrator_</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="IsotonicRegression"><a class="viewcode-back" href="../../automod/pycalib.calibration_methods.IsotonicRegression.html#pycalib.calibration_methods.IsotonicRegression">[docs]</a><span class="k">class</span> <span class="nc">IsotonicRegression</span><span class="p">(</span><span class="n">CalibrationMethod</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Probability calibration using Isotonic Regression</span>

<span class="sd">    Isotonic regression [1]_ [2]_ is a non-parametric approach to mapping (non-probabilistic) classifier scores to</span>
<span class="sd">    probabilities. It assumes an isotonic (non-decreasing) relationship between classifier scores and probabilities.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    out_of_bounds : string, optional, default: &quot;clip&quot;</span>
<span class="sd">        The ``out_of_bounds`` parameter handles how x-values outside of the</span>
<span class="sd">        training domain are handled.  When set to &quot;nan&quot;, predicted y-values</span>
<span class="sd">        will be NaN.  When set to &quot;clip&quot;, predicted y-values will be</span>
<span class="sd">        set to the value corresponding to the nearest train interval endpoint.</span>
<span class="sd">        When set to &quot;raise&quot;, allow ``interp1d`` to throw ValueError.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] Transforming Classifier Scores into Accurate Multiclass</span>
<span class="sd">           Probability Estimates, B. Zadrozny &amp; C. Elkan, (KDD 2002)</span>
<span class="sd">    .. [2] Predicting Good Probabilities with Supervised Learning,</span>
<span class="sd">           A. Niculescu-Mizil &amp; R. Caruana, ICML 2005</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">out_of_bounds</span><span class="o">=</span><span class="s2">&quot;clip&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_of_bounds</span> <span class="o">=</span> <span class="n">out_of_bounds</span>

<div class="viewcode-block" id="IsotonicRegression.fit"><a class="viewcode-back" href="../../automod/pycalib.calibration_methods.IsotonicRegression.html#pycalib.calibration_methods.IsotonicRegression.fit">[docs]</a>    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fit the calibration method based on the given uncalibrated class probabilities X and ground truth labels y.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like, shape (n_samples, n_classes)</span>
<span class="sd">            Training data, i.e. predicted probabilities of the base classifier on the calibration set.</span>
<span class="sd">        y : array-like, shape (n_samples,)</span>
<span class="sd">            Target classes.</span>
<span class="sd">        n_jobs : int or None, optional (default=None)</span>
<span class="sd">            The number of jobs to use for the computation.</span>
<span class="sd">            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.</span>
<span class="sd">            ``-1`` means using all processors.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self : object</span>
<span class="sd">            Returns an instance of self.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Calibration training data must have shape (n_samples, n_classes).&quot;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">X</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">isotonic_regressor_</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">isotonic</span><span class="o">.</span><span class="n">IsotonicRegression</span><span class="p">(</span><span class="n">increasing</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                                                           <span class="n">out_of_bounds</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">out_of_bounds</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">isotonic_regressor_</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">X</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">onevsrest_calibrator_</span> <span class="o">=</span> <span class="n">OneVsRestCalibrator</span><span class="p">(</span><span class="n">calibrator</span><span class="o">=</span><span class="n">clone</span><span class="p">(</span><span class="bp">self</span><span class="p">),</span> <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">onevsrest_calibrator_</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="IsotonicRegression.predict_proba"><a class="viewcode-back" href="../../automod/pycalib.calibration_methods.IsotonicRegression.html#pycalib.calibration_methods.IsotonicRegression.predict_proba">[docs]</a>    <span class="k">def</span> <span class="nf">predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute calibrated posterior probabilities for a given array of posterior probabilities from an arbitrary</span>
<span class="sd">        classifier.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like, shape (n_samples, n_classes)</span>
<span class="sd">            The uncalibrated posterior probabilities.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        P : array, shape (n_samples, n_classes)</span>
<span class="sd">            The predicted probabilities.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Calibration data must have shape (n_samples, n_classes).&quot;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">X</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;isotonic_regressor_&quot;</span><span class="p">)</span>
            <span class="n">p1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">isotonic_regressor_</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p1</span><span class="p">,</span> <span class="n">p1</span><span class="p">])</span>
        <span class="k">elif</span> <span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">X</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;onevsrest_calibrator_&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">onevsrest_calibrator_</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="BetaCalibration"><a class="viewcode-back" href="../../automod/pycalib.calibration_methods.BetaCalibration.html#pycalib.calibration_methods.BetaCalibration">[docs]</a><span class="k">class</span> <span class="nc">BetaCalibration</span><span class="p">(</span><span class="n">CalibrationMethod</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Probability calibration using Beta calibration</span>

<span class="sd">    Beta calibration [1]_ [2]_ is a parametric approach to calibration, specifically designed for probabilistic</span>
<span class="sd">    classifiers with output range [0,1]. Here, a calibration map family is defined based on the likelihood ratio between</span>
<span class="sd">    two Beta distributions. This parametric assumption is appropriate if the marginal class distributions follow Beta</span>
<span class="sd">    distributions. The beta calibration map has three parameters, two shape parameters `a` and `b` and one location</span>
<span class="sd">    parameter `m`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">        params : str, default=&quot;abm&quot;</span>
<span class="sd">            Defines which parameters to fit and which to hold constant. One of [&#39;abm&#39;, &#39;ab&#39;, &#39;am&#39;].</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] Kull, M., Silva Filho, T. M., Flach, P., et al. Beyond sigmoids: How to obtain well-calibrated probabilities</span>
<span class="sd">           from binary classifiers with beta calibration. Electronic Journal of Statistics 11, 5052–5080 (2017)</span>
<span class="sd">    .. [2] Kull, M., Filho, T. S. &amp; Flach, P. Beta calibration: a well-founded and easily implemented improvement on</span>
<span class="sd">           logistic calibration for binary classifiers in Proceedings of the 20th International Conference on Artificial</span>
<span class="sd">           Intelligence and Statistics (AISTATS)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="s2">&quot;abm&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="n">params</span>

<div class="viewcode-block" id="BetaCalibration.fit"><a class="viewcode-back" href="../../automod/pycalib.calibration_methods.BetaCalibration.html#pycalib.calibration_methods.BetaCalibration.fit">[docs]</a>    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fit the calibration method based on the given uncalibrated class probabilities X and ground truth labels y.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like, shape (n_samples, n_classes)</span>
<span class="sd">            Training data, i.e. predicted probabilities of the base classifier on the calibration set.</span>
<span class="sd">        y : array-like, shape (n_samples,)</span>
<span class="sd">            Target classes.</span>
<span class="sd">        n_jobs : int or None, optional (default=None)</span>
<span class="sd">            The number of jobs to use for the computation.</span>
<span class="sd">            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.</span>
<span class="sd">            ``-1`` means using all processors.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self : object</span>
<span class="sd">            Returns an instance of self.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Calibration training data must have shape (n_samples, n_classes).&quot;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">X</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">beta_calibrator_</span> <span class="o">=</span> <span class="n">betacal</span><span class="o">.</span><span class="n">BetaCalibration</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">beta_calibrator_</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">X</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">onevsrest_calibrator_</span> <span class="o">=</span> <span class="n">OneVsRestCalibrator</span><span class="p">(</span><span class="n">calibrator</span><span class="o">=</span><span class="n">clone</span><span class="p">(</span><span class="bp">self</span><span class="p">),</span> <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">onevsrest_calibrator_</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="BetaCalibration.predict_proba"><a class="viewcode-back" href="../../automod/pycalib.calibration_methods.BetaCalibration.html#pycalib.calibration_methods.BetaCalibration.predict_proba">[docs]</a>    <span class="k">def</span> <span class="nf">predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute calibrated posterior probabilities for a given array of posterior probabilities from an arbitrary</span>
<span class="sd">        classifier.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like, shape (n_samples, n_classes)</span>
<span class="sd">            The uncalibrated posterior probabilities.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        P : array, shape (n_samples, n_classes)</span>
<span class="sd">            The predicted probabilities.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Calibration data must have shape (n_samples, n_classes).&quot;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">X</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;beta_calibrator_&quot;</span><span class="p">)</span>
            <span class="n">p1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta_calibrator_</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p1</span><span class="p">,</span> <span class="n">p1</span><span class="p">])</span>
        <span class="k">elif</span> <span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">X</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;onevsrest_calibrator_&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">onevsrest_calibrator_</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="HistogramBinning"><a class="viewcode-back" href="../../automod/pycalib.calibration_methods.HistogramBinning.html#pycalib.calibration_methods.HistogramBinning">[docs]</a><span class="k">class</span> <span class="nc">HistogramBinning</span><span class="p">(</span><span class="n">CalibrationMethod</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Probability calibration using histogram binning</span>

<span class="sd">    Histogram binning [1]_ is a nonparametric approach to probability calibration. Classifier scores are binned into a given</span>
<span class="sd">    number of bins either based on fixed width or frequency. Classifier scores are then computed based on the empirical</span>
<span class="sd">    frequency of class 1 in each bin.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">        mode : str, default=&#39;equal_width&#39;</span>
<span class="sd">            Binning mode used. One of [&#39;equal_width&#39;, &#39;equal_freq&#39;].</span>
<span class="sd">        n_bins : int, default=20</span>
<span class="sd">            Number of bins to bin classifier scores into.</span>
<span class="sd">        input_range : list, shape (2,), default=[0, 1]</span>
<span class="sd">            Range of the classifier scores.</span>

<span class="sd">    .. [1] Zadrozny, B. &amp; Elkan, C. Obtaining calibrated probability estimates from decision trees and naive Bayesian</span>
<span class="sd">           classifiers in Proceedings of the 18th International Conference on Machine Learning (ICML, 2001), 609–616.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;equal_freq&#39;</span><span class="p">,</span> <span class="n">n_bins</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">input_range</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">mode</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;equal_width&#39;</span><span class="p">,</span> <span class="s1">&#39;equal_freq&#39;</span><span class="p">]:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">mode</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Mode not recognized. Choose on of &#39;equal_width&#39;, or &#39;equal_freq&#39;.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_bins</span> <span class="o">=</span> <span class="n">n_bins</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_range</span> <span class="o">=</span> <span class="n">input_range</span>

<div class="viewcode-block" id="HistogramBinning.fit"><a class="viewcode-back" href="../../automod/pycalib.calibration_methods.HistogramBinning.html#pycalib.calibration_methods.HistogramBinning.fit">[docs]</a>    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fit the calibration method based on the given uncalibrated class probabilities X and ground truth labels y.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like, shape (n_samples, n_classes)</span>
<span class="sd">            Training data, i.e. predicted probabilities of the base classifier on the calibration set.</span>
<span class="sd">        y : array-like, shape (n_samples,)</span>
<span class="sd">            Target classes.</span>
<span class="sd">        n_jobs : int or None, optional (default=None)</span>
<span class="sd">            The number of jobs to use for the computation.</span>
<span class="sd">            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.</span>
<span class="sd">            ``-1`` means using all processors.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self : object</span>
<span class="sd">            Returns an instance of self.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Calibration training data must have shape (n_samples, n_classes).&quot;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">X</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fit_binary</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">X</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">onevsrest_calibrator_</span> <span class="o">=</span> <span class="n">OneVsRestCalibrator</span><span class="p">(</span><span class="n">calibrator</span><span class="o">=</span><span class="n">clone</span><span class="p">(</span><span class="bp">self</span><span class="p">),</span> <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">onevsrest_calibrator_</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

    <span class="k">def</span> <span class="nf">_fit_binary</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;equal_width&#39;</span><span class="p">:</span>
            <span class="c1"># Compute probability of class 1 in each equal width bin</span>
            <span class="n">binned_stat</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">binned_statistic</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">values</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">statistic</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span>
                                                       <span class="n">bins</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_bins</span><span class="p">,</span> <span class="nb">range</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">input_range</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">prob_class_1</span> <span class="o">=</span> <span class="n">binned_stat</span><span class="o">.</span><span class="n">statistic</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">binning</span> <span class="o">=</span> <span class="n">binned_stat</span><span class="o">.</span><span class="n">bin_edges</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;equal_freq&#39;</span><span class="p">:</span>
            <span class="c1"># Find binning based on equal frequency</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">binning</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span>
                                       <span class="n">q</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_range</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_range</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_bins</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>

            <span class="c1"># Compute probability of class 1 in equal frequency bins</span>
            <span class="n">digitized</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">digitize</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">binning</span><span class="p">)</span>
            <span class="n">digitized</span><span class="p">[</span><span class="n">digitized</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">binning</span><span class="p">)]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">binning</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>  <span class="c1"># include rightmost edge in partition</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">prob_class_1</span> <span class="o">=</span> <span class="p">[</span><span class="n">y</span><span class="p">[</span><span class="n">digitized</span> <span class="o">==</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">binning</span><span class="p">))]</span>

        <span class="k">return</span> <span class="bp">self</span>

<div class="viewcode-block" id="HistogramBinning.predict_proba"><a class="viewcode-back" href="../../automod/pycalib.calibration_methods.HistogramBinning.html#pycalib.calibration_methods.HistogramBinning.predict_proba">[docs]</a>    <span class="k">def</span> <span class="nf">predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute calibrated posterior probabilities for a given array of posterior probabilities from an arbitrary</span>
<span class="sd">        classifier.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like, shape (n_samples, n_classes)</span>
<span class="sd">            The uncalibrated posterior probabilities.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        P : array, shape (n_samples, n_classes)</span>
<span class="sd">            The predicted probabilities.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Calibration data must have shape (n_samples, n_classes).&quot;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">X</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;binning&quot;</span><span class="p">,</span> <span class="s2">&quot;prob_class_1&quot;</span><span class="p">])</span>
            <span class="c1"># Find bin of predictions</span>
            <span class="n">digitized</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">digitize</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">binning</span><span class="p">)</span>
            <span class="n">digitized</span><span class="p">[</span><span class="n">digitized</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">binning</span><span class="p">)]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">binning</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>  <span class="c1"># include rightmost edge in partition</span>
            <span class="c1"># Transform to empirical frequency of class 1 in each bin</span>
            <span class="n">p1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">prob_class_1</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="p">(</span><span class="n">digitized</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)])</span>
            <span class="c1"># If empirical frequency is NaN, do not change prediction</span>
            <span class="n">p1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">p1</span><span class="p">),</span> <span class="n">p1</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
            <span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">p1</span><span class="p">)),</span> <span class="s2">&quot;Predictions are not all finite.&quot;</span>

            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p1</span><span class="p">,</span> <span class="n">p1</span><span class="p">])</span>
        <span class="k">elif</span> <span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">X</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;onevsrest_calibrator_&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">onevsrest_calibrator_</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="BayesianBinningQuantiles"><a class="viewcode-back" href="../../automod/pycalib.calibration_methods.BayesianBinningQuantiles.html#pycalib.calibration_methods.BayesianBinningQuantiles">[docs]</a><span class="k">class</span> <span class="nc">BayesianBinningQuantiles</span><span class="p">(</span><span class="n">CalibrationMethod</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Probability calibration using Bayesian binning into quantiles</span>

<span class="sd">    Bayesian binning into quantiles [1]_ considers multiple equal frequency binning models and combines them through</span>
<span class="sd">    Bayesian model averaging. Each binning model :math:`M` is scored according to</span>
<span class="sd">    :math:`\\text{Score}(M) = P(M) \\cdot P(D | M),` where a uniform prior :math:`P(M)` is assumed. The marginal likelihood</span>
<span class="sd">    :math:`P(D | M)` has a closed form solution under the assumption of independent binomial class distributions in each</span>
<span class="sd">    bin with beta priors.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">        C : int, default = 10</span>
<span class="sd">            Constant controlling the number of binning models.</span>
<span class="sd">        input_range : list, shape (2,), default=[0, 1]</span>
<span class="sd">            Range of the scores to calibrate.</span>

<span class="sd">    .. [1] Naeini, M. P., Cooper, G. F. &amp; Hauskrecht, M. Obtaining Well Calibrated Probabilities Using Bayesian Binning</span>
<span class="sd">           in Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, Austin, Texas, USA.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">input_range</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">C</span> <span class="o">=</span> <span class="n">C</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_range</span> <span class="o">=</span> <span class="n">input_range</span>

    <span class="k">def</span> <span class="nf">_binning_model_logscore</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">partition</span><span class="p">,</span> <span class="n">N_prime</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute the log score of a binning model</span>

<span class="sd">        Each binning model :math:`M` is scored according to :math:`Score(M) = P(M) \\cdot P(D | M),` where a uniform prior</span>
<span class="sd">        :math:`P(M)` is assumed and the marginal likelihood :math:`P(D | M)` has a closed form solution</span>
<span class="sd">        under the assumption of a binomial class distribution in each bin with beta priors.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        probs : array-like, shape (n_samples, )</span>
<span class="sd">            Predicted posterior probabilities.</span>
<span class="sd">        y : array-like, shape (n_samples, )</span>
<span class="sd">            Target classes.</span>
<span class="sd">        partition : array-like, shape (n_bins + 1, )</span>
<span class="sd">            Interval partition defining a binning.</span>
<span class="sd">        N_prime : int, default=2</span>
<span class="sd">            Equivalent sample size expressing the strength of the belief in the prior distribution.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        log_score : float</span>
<span class="sd">            Log of Bayesian score for a given binning model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Setup</span>
        <span class="n">B</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">partition</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="n">p</span> <span class="o">=</span> <span class="p">(</span><span class="n">partition</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">-</span> <span class="n">partition</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="o">/</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">partition</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># Compute positive and negative samples in given bins</span>
        <span class="n">N</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">histogram</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">partition</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">digitized</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">digitize</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">partition</span><span class="p">)</span>
        <span class="n">digitized</span><span class="p">[</span><span class="n">digitized</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">partition</span><span class="p">)]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">partition</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>  <span class="c1"># include rightmost edge in partition</span>
        <span class="n">m</span> <span class="o">=</span> <span class="p">[</span><span class="n">y</span><span class="p">[</span><span class="n">digitized</span> <span class="o">==</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">partition</span><span class="p">))]</span>
        <span class="n">n</span> <span class="o">=</span> <span class="n">N</span> <span class="o">-</span> <span class="n">m</span>

        <span class="c1"># Compute the parameters of the Beta priors</span>
        <span class="n">tiny</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float</span><span class="p">)</span><span class="o">.</span><span class="n">tiny</span>  <span class="c1"># Avoid scipy.special.gammaln(0), which can arise if bin has zero width</span>
        <span class="n">alpha</span> <span class="o">=</span> <span class="n">N_prime</span> <span class="o">/</span> <span class="n">B</span> <span class="o">*</span> <span class="n">p</span>
        <span class="n">alpha</span><span class="p">[</span><span class="n">alpha</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">tiny</span>
        <span class="n">beta</span> <span class="o">=</span> <span class="n">N_prime</span> <span class="o">/</span> <span class="n">B</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p</span><span class="p">)</span>
        <span class="n">beta</span><span class="p">[</span><span class="n">beta</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">tiny</span>

        <span class="c1"># Prior for a given binning model (uniform)</span>
        <span class="n">log_prior</span> <span class="o">=</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

        <span class="c1"># Compute the marginal log-likelihood for the given binning model</span>
        <span class="n">log_likelihood</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
            <span class="n">scipy</span><span class="o">.</span><span class="n">special</span><span class="o">.</span><span class="n">gammaln</span><span class="p">(</span><span class="n">N_prime</span> <span class="o">/</span> <span class="n">B</span><span class="p">)</span> <span class="o">+</span> <span class="n">scipy</span><span class="o">.</span><span class="n">special</span><span class="o">.</span><span class="n">gammaln</span><span class="p">(</span><span class="n">m</span> <span class="o">+</span> <span class="n">alpha</span><span class="p">)</span> <span class="o">+</span> <span class="n">scipy</span><span class="o">.</span><span class="n">special</span><span class="o">.</span><span class="n">gammaln</span><span class="p">(</span><span class="n">n</span> <span class="o">+</span> <span class="n">beta</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span>
                    <span class="n">scipy</span><span class="o">.</span><span class="n">special</span><span class="o">.</span><span class="n">gammaln</span><span class="p">(</span><span class="n">N</span> <span class="o">+</span> <span class="n">N_prime</span> <span class="o">/</span> <span class="n">B</span><span class="p">)</span> <span class="o">+</span> <span class="n">scipy</span><span class="o">.</span><span class="n">special</span><span class="o">.</span><span class="n">gammaln</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span> <span class="o">+</span> <span class="n">scipy</span><span class="o">.</span><span class="n">special</span><span class="o">.</span><span class="n">gammaln</span><span class="p">(</span>
                <span class="n">beta</span><span class="p">)))</span>

        <span class="c1"># Compute score for the given binning model</span>
        <span class="n">log_score</span> <span class="o">=</span> <span class="n">log_prior</span> <span class="o">+</span> <span class="n">log_likelihood</span>
        <span class="k">return</span> <span class="n">log_score</span>

<div class="viewcode-block" id="BayesianBinningQuantiles.fit"><a class="viewcode-back" href="../../automod/pycalib.calibration_methods.BayesianBinningQuantiles.html#pycalib.calibration_methods.BayesianBinningQuantiles.fit">[docs]</a>    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fit the calibration method based on the given uncalibrated class probabilities X and ground truth labels y.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like, shape (n_samples, n_classes)</span>
<span class="sd">            Training data, i.e. predicted probabilities of the base classifier on the calibration set.</span>
<span class="sd">        y : array-like, shape (n_samples,)</span>
<span class="sd">            Target classes.</span>
<span class="sd">        n_jobs : int or None, optional (default=None)</span>
<span class="sd">            The number of jobs to use for the computation.</span>
<span class="sd">            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.</span>
<span class="sd">            ``-1`` means using all processors.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self : object</span>
<span class="sd">            Returns an instance of self.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Calibration training data must have shape (n_samples, n_classes).&quot;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">X</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">binnings</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">log_scores</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">prob_class_1</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">T</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fit_binary</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">X</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">onevsrest_calibrator_</span> <span class="o">=</span> <span class="n">OneVsRestCalibrator</span><span class="p">(</span><span class="n">calibrator</span><span class="o">=</span><span class="n">clone</span><span class="p">(</span><span class="bp">self</span><span class="p">),</span> <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">onevsrest_calibrator_</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
            <span class="k">return</span> <span class="bp">self</span></div>

    <span class="k">def</span> <span class="nf">_fit_binary</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="c1"># Determine number of bins</span>
        <span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">min_bins</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">N</span> <span class="o">**</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="mi">3</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">C</span><span class="p">)))</span>
        <span class="n">max_bins</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">N</span> <span class="o">/</span> <span class="mi">5</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">C</span> <span class="o">*</span> <span class="n">N</span> <span class="o">**</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="mi">3</span><span class="p">))))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">T</span> <span class="o">=</span> <span class="n">max_bins</span> <span class="o">-</span> <span class="n">min_bins</span> <span class="o">+</span> <span class="mi">1</span>

        <span class="c1"># Define (equal frequency) binning models and compute scores</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">binnings</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_scores</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prob_class_1</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">n_bins</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">min_bins</span><span class="p">,</span> <span class="n">max_bins</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)):</span>
            <span class="c1"># Compute binning from data and set outer edges to range</span>
            <span class="n">binning_tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">q</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_range</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_range</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">n_bins</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
            <span class="n">binning_tmp</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_range</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">binning_tmp</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_range</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="c1"># Enforce monotonicity of binning (np.quantile does not guarantee monotonicity)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">binnings</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="o">.</span><span class="n">accumulate</span><span class="p">(</span><span class="n">binning_tmp</span><span class="p">))</span>
            <span class="c1"># Compute score</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">log_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_binning_model_logscore</span><span class="p">(</span><span class="n">probs</span><span class="o">=</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">partition</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">binnings</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>

            <span class="c1"># Compute empirical accuracy for all bins</span>
            <span class="n">digitized</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">digitize</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">binnings</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="c1"># include rightmost edge in partition</span>
            <span class="n">digitized</span><span class="p">[</span><span class="n">digitized</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">binnings</span><span class="p">[</span><span class="n">i</span><span class="p">])]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">binnings</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">-</span> <span class="mi">1</span>

            <span class="k">def</span> <span class="nf">empty_safe_bin_mean</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">empty_value</span><span class="p">):</span>
                <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">                Assign the bin mean to an empty bin. Corresponds to prior assumption of the underlying classifier</span>
<span class="sd">                being calibrated.</span>
<span class="sd">                &quot;&quot;&quot;</span>
                <span class="k">if</span> <span class="n">a</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="k">return</span> <span class="n">empty_value</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">return</span> <span class="n">a</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">prob_class_1</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="p">[</span><span class="n">empty_safe_bin_mean</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">digitized</span> <span class="o">==</span> <span class="n">k</span><span class="p">],</span> <span class="n">empty_value</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">binnings</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">binnings</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">k</span> <span class="o">-</span> <span class="mi">1</span><span class="p">])</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
                 <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">binnings</span><span class="p">[</span><span class="n">i</span><span class="p">]))])</span>

        <span class="k">return</span> <span class="bp">self</span>

<div class="viewcode-block" id="BayesianBinningQuantiles.predict_proba"><a class="viewcode-back" href="../../automod/pycalib.calibration_methods.BayesianBinningQuantiles.html#pycalib.calibration_methods.BayesianBinningQuantiles.predict_proba">[docs]</a>    <span class="k">def</span> <span class="nf">predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute calibrated posterior probabilities for a given array of posterior probabilities from an arbitrary</span>
<span class="sd">        classifier.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like, shape (n_samples, n_classes)</span>
<span class="sd">            The uncalibrated posterior probabilities.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        P : array, shape (n_samples, n_classes)</span>
<span class="sd">            The predicted probabilities.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Calibration data must have shape (n_samples, n_classes).&quot;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">X</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;binnings&quot;</span><span class="p">,</span> <span class="s2">&quot;log_scores&quot;</span><span class="p">,</span> <span class="s2">&quot;prob_class_1&quot;</span><span class="p">,</span> <span class="s2">&quot;T&quot;</span><span class="p">])</span>

            <span class="c1"># Find bin for all binnings and the associated empirical accuracy</span>
            <span class="n">posterior_prob_binnings</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">X</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">binnings</span><span class="p">)])</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">binning</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">binnings</span><span class="p">):</span>
                <span class="n">bin_ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">searchsorted</span><span class="p">(</span><span class="n">binning</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
                <span class="n">bin_ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">bin_ids</span><span class="p">,</span> <span class="n">a_min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">a_max</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">binning</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># necessary if X is out of range</span>
                <span class="n">posterior_prob_binnings</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">prob_class_1</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="p">(</span><span class="n">bin_ids</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span>

            <span class="c1"># Computed score-weighted average</span>
            <span class="n">norm_weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">log_scores</span><span class="p">)</span> <span class="o">-</span> <span class="n">scipy</span><span class="o">.</span><span class="n">special</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">log_scores</span><span class="p">))</span>
            <span class="n">posterior_prob</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">posterior_prob_binnings</span> <span class="o">*</span> <span class="n">norm_weights</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

            <span class="c1"># Compute probability for other class</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span><span class="mi">1</span> <span class="o">-</span> <span class="n">posterior_prob</span><span class="p">,</span> <span class="n">posterior_prob</span><span class="p">])</span>
        <span class="k">elif</span> <span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">X</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;onevsrest_calibrator_&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">onevsrest_calibrator_</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="GPCalibration"><a class="viewcode-back" href="../../automod/pycalib.calibration_methods.GPCalibration.html#pycalib.calibration_methods.GPCalibration">[docs]</a><span class="k">class</span> <span class="nc">GPCalibration</span><span class="p">(</span><span class="n">CalibrationMethod</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Probability calibration using a latent Gaussian process</span>

<span class="sd">    Gaussian process calibration [1]_ is a non-parametric approach to calibrate posterior probabilities from an arbitrary</span>
<span class="sd">    classifier based on a hold-out data set. Inference is performed using a sparse variational Gaussian process</span>
<span class="sd">    (SVGP) [2]_ implemented in `gpflow` [3]_.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_classes : int</span>
<span class="sd">        Number of classes in calibration data.</span>
<span class="sd">    logits : bool, default=False</span>
<span class="sd">        Are the inputs for calibration logits (e.g. from a neural network)?</span>
<span class="sd">    mean_function : GPflow object</span>
<span class="sd">        Mean function of the latent GP.</span>
<span class="sd">    kernel : GPflow object</span>
<span class="sd">        Kernel function of the latent GP.</span>
<span class="sd">    likelihood : GPflow object</span>
<span class="sd">        Likelihood giving a prior on the class prediction.</span>
<span class="sd">    n_inducing_points : int, default=100</span>
<span class="sd">        Number of inducing points for the variational approximation.</span>
<span class="sd">    maxiter : int, default=1000</span>
<span class="sd">        Maximum number of iterations for the likelihood optimization procedure.</span>
<span class="sd">    n_monte_carlo : int, default=100</span>
<span class="sd">        Number of Monte Carlo samples for the inference procedure.</span>
<span class="sd">    max_samples_monte_carlo : int, default=10**7</span>
<span class="sd">        Maximum number of Monte Carlo samples to draw in one batch when predicting. Setting this value too large can</span>
<span class="sd">        cause memory issues.</span>
<span class="sd">    inf_mean_approx : bool, default=False</span>
<span class="sd">        If True, when inferring calibrated probabilities, only the mean of the latent Gaussian process is taken into</span>
<span class="sd">        account, not its covariance.</span>
<span class="sd">    session : tf.Session, default=None</span>
<span class="sd">        `tensorflow` session to use.</span>
<span class="sd">    random_state : int, default=0</span>
<span class="sd">        Random seed for reproducibility. Needed for Monte-Carlo sampling routine.</span>
<span class="sd">    verbose : bool</span>
<span class="sd">        Print information on optimization routine.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] Wenger, J., Kjellström H. &amp; Triebel, R. Non-Parametric Calibration for Classification in</span>
<span class="sd">           Proceedings of AISTATS (2020)</span>
<span class="sd">    .. [2] Hensman, J., Matthews, A. G. d. G. &amp; Ghahramani, Z. Scalable Variational Gaussian Process Classification in</span>
<span class="sd">           Proceedings of AISTATS (2015)</span>
<span class="sd">    .. [3] Matthews, A. G. d. G., van der Wilk, M., et al. GPflow: A Gaussian process library using TensorFlow. Journal</span>
<span class="sd">           of Machine Learning Research 18, 1–6 (Apr. 2017)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">n_classes</span><span class="p">,</span>
                 <span class="n">logits</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">mean_function</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">kernel</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">likelihood</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">n_inducing_points</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                 <span class="n">maxiter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
                 <span class="n">n_monte_carlo</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                 <span class="n">max_samples_monte_carlo</span><span class="o">=</span><span class="mi">10</span> <span class="o">**</span> <span class="mi">7</span><span class="p">,</span>
                 <span class="n">inf_mean_approx</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">session</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># Parameter initialization</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_classes</span> <span class="o">=</span> <span class="n">n_classes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>

        <span class="c1"># Initialization of tensorflow session</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">session</span> <span class="o">=</span> <span class="n">session</span>

        <span class="c1"># Initialization of Gaussian process components and inference parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">n_classes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_monte_carlo</span> <span class="o">=</span> <span class="n">n_monte_carlo</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_samples_monte_carlo</span> <span class="o">=</span> <span class="n">max_samples_monte_carlo</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_inducing_points</span> <span class="o">=</span> <span class="n">n_inducing_points</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">maxiter</span> <span class="o">=</span> <span class="n">maxiter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inf_mean_approx</span> <span class="o">=</span> <span class="n">inf_mean_approx</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="o">=</span> <span class="n">random_state</span>
        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>  <span class="c1"># Set seed for optimization of hyperparameters</span>

        <span class="k">with</span> <span class="n">gpflow</span><span class="o">.</span><span class="n">defer_build</span><span class="p">():</span>

            <span class="c1"># Set likelihood</span>
            <span class="k">if</span> <span class="n">likelihood</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">likelihood</span> <span class="o">=</span> <span class="n">gp_classes</span><span class="o">.</span><span class="n">MultiCal</span><span class="p">(</span><span class="n">num_classes</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_classes</span><span class="p">,</span>
                                                      <span class="n">num_monte_carlo_points</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_monte_carlo</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">likelihood</span> <span class="o">=</span> <span class="n">likelihood</span>

            <span class="c1"># Set mean function</span>
            <span class="k">if</span> <span class="n">mean_function</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">logits</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">mean_function</span> <span class="o">=</span> <span class="n">gpflow</span><span class="o">.</span><span class="n">conditionals</span><span class="o">.</span><span class="n">mean_functions</span><span class="o">.</span><span class="n">Identity</span><span class="p">()</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">mean_function</span> <span class="o">=</span> <span class="n">gp_classes</span><span class="o">.</span><span class="n">Log</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">mean_function</span> <span class="o">=</span> <span class="n">mean_function</span>

            <span class="c1"># Set kernel</span>
            <span class="k">if</span> <span class="n">kernel</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">k_white</span> <span class="o">=</span> <span class="n">gpflow</span><span class="o">.</span><span class="n">kernels</span><span class="o">.</span><span class="n">White</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">variance</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">logits</span><span class="p">:</span>
                    <span class="n">kernel_lengthscale</span> <span class="o">=</span> <span class="mi">10</span>
                    <span class="n">k_rbf</span> <span class="o">=</span> <span class="n">gpflow</span><span class="o">.</span><span class="n">kernels</span><span class="o">.</span><span class="n">RBF</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">lengthscales</span><span class="o">=</span><span class="n">kernel_lengthscale</span><span class="p">,</span> <span class="n">variance</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">kernel_lengthscale</span> <span class="o">=</span> <span class="mf">0.5</span>
                    <span class="n">k_rbf</span> <span class="o">=</span> <span class="n">gpflow</span><span class="o">.</span><span class="n">kernels</span><span class="o">.</span><span class="n">RBF</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">lengthscales</span><span class="o">=</span><span class="n">kernel_lengthscale</span><span class="p">,</span> <span class="n">variance</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                    <span class="c1"># Place constraints [a,b] on kernel parameters</span>
                    <span class="n">k_rbf</span><span class="o">.</span><span class="n">lengthscales</span><span class="o">.</span><span class="n">transform</span> <span class="o">=</span> <span class="n">gpflow</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">Logistic</span><span class="p">(</span><span class="n">a</span><span class="o">=.</span><span class="mi">001</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
                    <span class="n">k_rbf</span><span class="o">.</span><span class="n">variance</span><span class="o">.</span><span class="n">transform</span> <span class="o">=</span> <span class="n">gpflow</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">Logistic</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="n">k_rbf</span> <span class="o">+</span> <span class="n">k_white</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="n">kernel</span>

<div class="viewcode-block" id="GPCalibration.fit"><a class="viewcode-back" href="../../automod/pycalib.calibration_methods.GPCalibration.html#pycalib.calibration_methods.GPCalibration.fit">[docs]</a>    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fit the calibration method based on the given uncalibrated class probabilities or logits X and ground truth</span>
<span class="sd">        labels y.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like, shape (n_samples, n_classes)</span>
<span class="sd">            Training data, i.e. predicted probabilities or logits of the base classifier on the calibration set.</span>
<span class="sd">        y : array-like, shape (n_samples,)</span>
<span class="sd">            Target classes.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self : object</span>
<span class="sd">            Returns an instance of self.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Check for correct dimensions</span>
        <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">X</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_classes</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Calibration data must have shape (n_samples, n_classes).&quot;</span><span class="p">)</span>

        <span class="c1"># Create a new TF session if none is given</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">session</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">session</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">(</span><span class="n">graph</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">Graph</span><span class="p">())</span>

        <span class="c1"># Fit GP in TF session</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="o">.</span><span class="n">as_default</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Calibration training data must have shape (n_samples, n_classes).&quot;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_fit_multiclass</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

    <span class="k">def</span> <span class="nf">_fit_multiclass</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="c1"># Setup</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Select inducing points through scipy.cluster.vq.kmeans</span>
        <span class="n">Z</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">vq</span><span class="o">.</span><span class="n">kmeans</span><span class="p">(</span><span class="n">obs</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                                    <span class="n">k_or_guess</span><span class="o">=</span><span class="nb">min</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_inducing_points</span><span class="p">,</span> <span class="p">))[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># Define SVGP calibration model with multiclass softargmax calibration likelihood</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">gp_classes</span><span class="o">.</span><span class="n">SVGPcal</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">Z</span><span class="o">=</span><span class="n">Z</span><span class="p">,</span>
                                        <span class="n">mean_function</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">mean_function</span><span class="p">,</span>
                                        <span class="n">kern</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="p">,</span>
                                        <span class="n">likelihood</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">likelihood</span><span class="p">,</span>
                                        <span class="n">whiten</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                        <span class="n">q_diag</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># Optimize parameters</span>
        <span class="n">opt</span> <span class="o">=</span> <span class="n">gpflow</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">ScipyOptimizer</span><span class="p">()</span>
        <span class="n">opt</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">maxiter</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">maxiter</span><span class="p">,</span> <span class="n">disp</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span>

<div class="viewcode-block" id="GPCalibration.predict_proba"><a class="viewcode-back" href="../../automod/pycalib.calibration_methods.GPCalibration.html#pycalib.calibration_methods.GPCalibration.predict_proba">[docs]</a>    <span class="k">def</span> <span class="nf">predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">mean_approx</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute calibrated posterior probabilities for a given array of posterior probabilities from an arbitrary</span>
<span class="sd">        classifier.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like, shape=(n_samples, n_classes)</span>
<span class="sd">            The uncalibrated posterior probabilities.</span>
<span class="sd">        mean_approx : bool, default=False</span>
<span class="sd">            If True, inference is performed using only the mean of the latent Gaussian process, not its covariance.</span>
<span class="sd">            Note, if `self.inference_mean_approximation==True`, then the logical value of this option is not considered.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        P : array, shape (n_samples, n_classes)</span>
<span class="sd">            The predicted probabilities.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;model&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">mean_approx</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">inf_mean_approx</span><span class="p">:</span>

            <span class="c1"># Evaluate latent GP</span>
            <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="o">.</span><span class="n">as_default</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
                <span class="n">f</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict_f</span><span class="p">(</span><span class="n">X_onedim</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
                <span class="n">latent</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>

            <span class="c1"># Return softargmax of fitted GP at input</span>
            <span class="k">return</span> <span class="n">scipy</span><span class="o">.</span><span class="n">special</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">latent</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>

            <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="o">.</span><span class="n">as_default</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
                <span class="c1"># Seed for Monte_Carlo</span>
                <span class="n">tf</span><span class="o">.</span><span class="n">set_random_seed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>

                <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">X</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_classes</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Calibration data must have shape (n_samples, n_classes).&quot;</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># Predict in batches to keep memory usage in Monte-Carlo sampling low</span>
                    <span class="n">n_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">X</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
                    <span class="n">samples_monte_carlo</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_classes</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_monte_carlo</span> <span class="o">*</span> <span class="n">n_data</span>
                    <span class="k">if</span> <span class="n">samples_monte_carlo</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_samples_monte_carlo</span><span class="p">:</span>
                        <span class="n">n_pred_batches</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">divmod</span><span class="p">(</span><span class="n">samples_monte_carlo</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_samples_monte_carlo</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">n_pred_batches</span> <span class="o">=</span> <span class="mi">1</span>

                    <span class="n">p_pred_list</span> <span class="o">=</span> <span class="p">[]</span>
                    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_pred_batches</span><span class="p">):</span>
                        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Predicting batch </span><span class="si">{}</span><span class="s2">/</span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_pred_batches</span><span class="p">))</span>
                        <span class="n">ind_range</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_samples_monte_carlo</span> <span class="o">*</span> <span class="n">i</span><span class="p">,</span>
                                              <span class="n">stop</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_samples_monte_carlo</span> <span class="o">*</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">n_data</span><span class="p">))</span>
                        <span class="n">p_pred_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict_full_density</span><span class="p">(</span><span class="n">Xnew</span><span class="o">=</span><span class="n">X</span><span class="p">[</span><span class="n">ind_range</span><span class="p">,</span> <span class="p">:]))</span><span class="o">.</span><span class="n">eval</span><span class="p">())</span>

                    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">p_pred_list</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span></div>

<div class="viewcode-block" id="GPCalibration.latent"><a class="viewcode-back" href="../../automod/pycalib.calibration_methods.GPCalibration.html#pycalib.calibration_methods.GPCalibration.latent">[docs]</a>    <span class="k">def</span> <span class="nf">latent</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Evaluate the latent function f(z) of the GP calibration method.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        z : array-like, shape=(n_evaluations,)</span>
<span class="sd">            Input confidence for which to evaluate the latent function.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        f : array-like, shape=(n_evaluations,)</span>
<span class="sd">            Values of the latent function at z.</span>
<span class="sd">        f_var : array-like, shape=(n_evaluations,)</span>
<span class="sd">            Variance of the latent function at z.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Evaluate latent GP</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="o">.</span><span class="n">as_default</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
            <span class="n">f</span><span class="p">,</span> <span class="n">var</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict_f</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
            <span class="n">latent</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
            <span class="n">latent_var</span> <span class="o">=</span> <span class="n">var</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">latent</span><span class="p">,</span> <span class="n">latent_var</span></div>

<div class="viewcode-block" id="GPCalibration.plot_latent"><a class="viewcode-back" href="../../automod/pycalib.calibration_methods.GPCalibration.html#pycalib.calibration_methods.GPCalibration.plot_latent">[docs]</a>    <span class="k">def</span> <span class="nf">plot_latent</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">filename</span><span class="p">,</span> <span class="n">plot_classes</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Plot the latent function of the calibration method.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        z : array-like, shape=(n_evaluations,)</span>
<span class="sd">            Input confidence to plot latent function for.</span>
<span class="sd">        filename :</span>
<span class="sd">            Filename / -path where to save output.</span>
<span class="sd">        plot_classes : bool, default=True</span>
<span class="sd">            Should classes also be plotted?</span>
<span class="sd">        kwargs</span>
<span class="sd">            Additional arguments passed on to matplotlib.pyplot.subplots.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Evaluate latent GP</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="o">.</span><span class="n">as_default</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
            <span class="n">f</span><span class="p">,</span> <span class="n">var</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict_f</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
            <span class="n">latent</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
            <span class="n">latent_var</span> <span class="o">=</span> <span class="n">var</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
            <span class="n">Z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">value</span>

        <span class="c1"># Plot latent GP</span>
        <span class="k">if</span> <span class="n">plot_classes</span><span class="p">:</span>
            <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">pycalib</span><span class="o">.</span><span class="n">texfig</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">latent</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;GP mean&quot;</span><span class="p">)</span>
            <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">latent</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">latent_var</span><span class="p">),</span> <span class="n">latent</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">latent_var</span><span class="p">),</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;GP $g(</span><span class="se">\\</span><span class="s2">textnormal</span><span class="si">{z}</span><span class="s2">_k)$&quot;</span><span class="p">)</span>
            <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">Z</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">Z</span><span class="p">),)),</span>
                         <span class="n">np</span><span class="o">.</span><span class="n">matlib</span><span class="o">.</span><span class="n">repmat</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_classes</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">Z</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">Z</span><span class="p">),)),</span> <span class="s1">&#39;kx&#39;</span><span class="p">,</span>
                         <span class="n">markersize</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
            <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;class $k$&quot;</span><span class="p">)</span>
            <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;confidence $</span><span class="se">\\</span><span class="s2">textnormal</span><span class="si">{z}</span><span class="s2">_k$&quot;</span><span class="p">)</span>
            <span class="n">fig</span><span class="o">.</span><span class="n">align_labels</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">pycalib</span><span class="o">.</span><span class="n">texfig</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="n">axes</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">latent</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;GP mean&quot;</span><span class="p">)</span>
            <span class="n">axes</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">latent</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">latent_var</span><span class="p">),</span> <span class="n">latent</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">latent_var</span><span class="p">),</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">axes</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;GP $g(</span><span class="se">\\</span><span class="s2">textnormal</span><span class="si">{z}</span><span class="s2">_k)$&quot;</span><span class="p">)</span>
            <span class="n">axes</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;confidence $</span><span class="se">\\</span><span class="s2">textnormal</span><span class="si">{z}</span><span class="s2">_k$&quot;</span><span class="p">)</span>

        <span class="c1"># Save plot to file</span>
        <span class="n">pycalib</span><span class="o">.</span><span class="n">texfig</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span></div></div>


<div class="viewcode-block" id="OneVsRestCalibrator"><a class="viewcode-back" href="../../automod/pycalib.calibration_methods.OneVsRestCalibrator.html#pycalib.calibration_methods.OneVsRestCalibrator">[docs]</a><span class="k">class</span> <span class="nc">OneVsRestCalibrator</span><span class="p">(</span><span class="n">sklearn</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">BaseEstimator</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;One-vs-the-rest (OvR) multiclass strategy</span>
<span class="sd">    Also known as one-vs-all, this strategy consists in fitting one calibrator</span>
<span class="sd">    per class. The probabilities to be calibrated of the other classes are summed.</span>
<span class="sd">    For each calibrator, the class is fitted against all the other classes.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    calibrator : CalibrationMethod object</span>
<span class="sd">        A CalibrationMethod object implementing `fit` and `predict_proba`.</span>
<span class="sd">    n_jobs : int or None, optional (default=None)</span>
<span class="sd">        The number of jobs to use for the computation.</span>
<span class="sd">        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.</span>
<span class="sd">        ``-1`` means using all processors. See :term:`Glossary &lt;n_jobs&gt;`</span>
<span class="sd">        for more details.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    calibrators_ : list of `n_classes` estimators</span>
<span class="sd">        Estimators used for predictions.</span>
<span class="sd">    classes_ : array, shape = [`n_classes`]</span>
<span class="sd">        Class labels.</span>
<span class="sd">    label_binarizer_ : LabelBinarizer object</span>
<span class="sd">        Object used to transform multiclass labels to binary labels and</span>
<span class="sd">        vice-versa.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">calibrator</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">calibrator</span> <span class="o">=</span> <span class="n">calibrator</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span> <span class="o">=</span> <span class="n">n_jobs</span>

<div class="viewcode-block" id="OneVsRestCalibrator.fit"><a class="viewcode-back" href="../../automod/pycalib.calibration_methods.OneVsRestCalibrator.html#pycalib.calibration_methods.OneVsRestCalibrator.fit">[docs]</a>    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Fit underlying estimators.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : (sparse) array-like, shape = [n_samples, n_features]</span>
<span class="sd">            Calibration data.</span>
<span class="sd">        y : (sparse) array-like, shape = [n_samples, ]</span>
<span class="sd">            Multi-class labels.</span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># A sparse LabelBinarizer, with sparse_output=True, has been shown to</span>
        <span class="c1"># outperform or match a dense label binarizer in all cases and has also</span>
        <span class="c1"># resulted in less or equal memory consumption in the fit_ovr function</span>
        <span class="c1"># overall.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">label_binarizer_</span> <span class="o">=</span> <span class="n">LabelBinarizer</span><span class="p">(</span><span class="n">sparse_output</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_binarizer_</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">Y</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">tocsc</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_binarizer_</span><span class="o">.</span><span class="n">classes_</span>
        <span class="n">columns</span> <span class="o">=</span> <span class="p">(</span><span class="n">col</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">Y</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
        <span class="c1"># In cases where individual estimators are very fast to train setting</span>
        <span class="c1"># n_jobs &gt; 1 in can results in slower performance due to the overhead</span>
        <span class="c1"># of spawning threads.  See joblib issue #112.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">calibrators_</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span><span class="p">)(</span>
            <span class="n">delayed</span><span class="p">(</span><span class="n">OneVsRestCalibrator</span><span class="o">.</span><span class="n">_fit_binary</span><span class="p">)(</span><span class="bp">self</span><span class="o">.</span><span class="n">calibrator</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">column</span><span class="p">,</span> <span class="n">classes</span><span class="o">=</span><span class="p">[</span>
                <span class="s2">&quot;not </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_binarizer_</span><span class="o">.</span><span class="n">classes_</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_binarizer_</span><span class="o">.</span><span class="n">classes_</span><span class="p">[</span><span class="n">i</span><span class="p">]])</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">column</span> <span class="ow">in</span>
            <span class="nb">enumerate</span><span class="p">(</span><span class="n">columns</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="OneVsRestCalibrator.predict_proba"><a class="viewcode-back" href="../../automod/pycalib.calibration_methods.OneVsRestCalibrator.html#pycalib.calibration_methods.OneVsRestCalibrator.predict_proba">[docs]</a>    <span class="k">def</span> <span class="nf">predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Probability estimates.</span>

<span class="sd">        The returned estimates for all classes are ordered by label of classes.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like, shape = [n_samples, n_features]</span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        T : (sparse) array-like, shape = [n_samples, n_classes]</span>
<span class="sd">            Returns the probability of the sample for each class in the model,</span>
<span class="sd">            where classes are ordered as they are in `self.classes_`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;classes_&quot;</span><span class="p">,</span> <span class="s2">&quot;calibrators_&quot;</span><span class="p">])</span>

        <span class="c1"># Y[i, j] gives the probability that sample i has the label j.</span>
        <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">c</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">delete</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">obj</span><span class="o">=</span><span class="n">i</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">X</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">[</span><span class="n">i</span><span class="p">]]]))[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span>
                      <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">calibrators_</span><span class="p">)])</span><span class="o">.</span><span class="n">T</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">calibrators_</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># Only one estimator, but we still want to return probabilities for two classes.</span>
            <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">Y</span><span class="p">),</span> <span class="n">Y</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Pad with zeros for classes not in training data</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">Y</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">X</span><span class="p">)[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="n">p_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
            <span class="n">p_pred</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">]</span> <span class="o">=</span> <span class="n">Y</span>
            <span class="n">Y</span> <span class="o">=</span> <span class="n">p_pred</span>

        <span class="c1"># Normalize probabilities to 1.</span>
        <span class="n">Y</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">preprocessing</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_norm</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">a_min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">a_max</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span></div>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">n_classes_</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">_first_calibrator</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">calibrators_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_fit_binary</span><span class="p">(</span><span class="n">calibrator</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">classes</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fit a single binary calibrator.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        calibrator</span>
<span class="sd">        X</span>
<span class="sd">        y</span>
<span class="sd">        classes</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Sum probabilities of combined classes in calibration training data X</span>
        <span class="n">cl</span> <span class="o">=</span> <span class="n">classes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">delete</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">cl</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">X</span><span class="p">[:,</span> <span class="n">cl</span><span class="p">]])</span>

        <span class="c1"># Check whether only one label is present in training data</span>
        <span class="n">unique_y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">unique_y</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">classes</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
                    <span class="n">c</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">c</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Label </span><span class="si">%s</span><span class="s2"> is present in all training examples.&quot;</span> <span class="o">%</span>
                              <span class="nb">str</span><span class="p">(</span><span class="n">classes</span><span class="p">[</span><span class="n">c</span><span class="p">]))</span>
            <span class="n">calibrator</span> <span class="o">=</span> <span class="n">_ConstantCalibrator</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">unique_y</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">calibrator</span> <span class="o">=</span> <span class="n">clone</span><span class="p">(</span><span class="n">calibrator</span><span class="p">)</span>
            <span class="n">calibrator</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">calibrator</span></div>


<span class="k">class</span> <span class="nc">_ConstantCalibrator</span><span class="p">(</span><span class="n">CalibrationMethod</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y_</span> <span class="o">=</span> <span class="n">y</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;y_&#39;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">y_</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;y_&#39;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">repeat</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_</span><span class="p">])],</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, Jonathan Wenger

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>