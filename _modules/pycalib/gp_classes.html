

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>pycalib.gp_classes &mdash; pycalib  documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/graphviz.css" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home"> pycalib
          

          
          </a>

          
            
            
              <div class="version">
                0.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../intro.html">Non-Parametric Calibration for Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../modules.html">Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../examples.html">Examples</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">pycalib</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../index.html">Module code</a> &raquo;</li>
        
      <li>pycalib.gp_classes</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for pycalib.gp_classes</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;Gaussian Process classes enabling variational inference of a single GP in GPcalibration.&quot;&quot;&quot;</span>

<span class="c1"># standard imports</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">gpflow</span>

<span class="c1"># Turn off tensorflow deprecation warnings</span>
<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="kn">import</span> <span class="n">module_wrapper</span> <span class="k">as</span> <span class="n">deprecation</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="kn">import</span> <span class="n">deprecation_wrapper</span> <span class="k">as</span> <span class="n">deprecation</span>
<span class="n">deprecation</span><span class="o">.</span><span class="n">_PER_MODULE_WARNING_LIMIT</span> <span class="o">=</span> <span class="mi">0</span>

<span class="c1"># gpflow imports</span>
<span class="kn">from</span> <span class="nn">gpflow.mean_functions</span> <span class="kn">import</span> <span class="n">MeanFunction</span>
<span class="kn">from</span> <span class="nn">gpflow</span> <span class="kn">import</span> <span class="n">features</span>
<span class="kn">from</span> <span class="nn">gpflow.conditionals</span> <span class="kn">import</span> <span class="n">conditional</span><span class="p">,</span> <span class="n">Kuu</span>
<span class="kn">from</span> <span class="nn">gpflow</span> <span class="kn">import</span> <span class="n">settings</span>
<span class="kn">from</span> <span class="nn">gpflow.decors</span> <span class="kn">import</span> <span class="n">params_as_tensors</span>
<span class="kn">from</span> <span class="nn">gpflow.quadrature</span> <span class="kn">import</span> <span class="n">ndiag_mc</span>
<span class="kn">from</span> <span class="nn">gpflow.params</span> <span class="kn">import</span> <span class="n">Parameter</span><span class="p">,</span> <span class="n">DataHolder</span><span class="p">,</span> <span class="n">Parameterized</span>
<span class="kn">from</span> <span class="nn">gpflow.models.model</span> <span class="kn">import</span> <span class="n">GPModel</span>
<span class="kn">from</span> <span class="nn">gpflow</span> <span class="kn">import</span> <span class="n">transforms</span><span class="p">,</span> <span class="n">kullback_leiblers</span>
<span class="kn">from</span> <span class="nn">gpflow.models.svgp</span> <span class="kn">import</span> <span class="n">Minibatch</span>


<span class="c1">############################</span>
<span class="c1">#    Mean Functions</span>
<span class="c1">############################</span>

<div class="viewcode-block" id="Log"><a class="viewcode-back" href="../../automod/pycalib.gp_classes.Log.html#pycalib.gp_classes.Log">[docs]</a><span class="k">class</span> <span class="nc">Log</span><span class="p">(</span><span class="n">MeanFunction</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Natural logarithm prior mean function.</span>

<span class="sd">    :math:`y_i = \log(x_i)`</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">MeanFunction</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

<div class="viewcode-block" id="Log.__call__"><a class="viewcode-back" href="../../automod/pycalib.gp_classes.Log.html#pycalib.gp_classes.Log.__call__">[docs]</a>    <span class="nd">@params_as_tensors</span>
    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="c1"># Avoid -inf = log(0)</span>
        <span class="n">tiny</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float</span><span class="p">)</span><span class="o">.</span><span class="n">tiny</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">clip_value_min</span><span class="o">=</span><span class="n">tiny</span><span class="p">,</span> <span class="n">clip_value_max</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">)</span>
        <span class="c1"># Returns the natural logarithm of the input</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">X</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="ScalarMult"><a class="viewcode-back" href="../../automod/pycalib.gp_classes.ScalarMult.html#pycalib.gp_classes.ScalarMult">[docs]</a><span class="k">class</span> <span class="nc">ScalarMult</span><span class="p">(</span><span class="n">MeanFunction</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Scalar multiplication mean function.</span>

<span class="sd">    :math:`y_i = \\alpha x_i`</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">MeanFunction</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">settings</span><span class="o">.</span><span class="n">float_type</span><span class="p">)</span>

<div class="viewcode-block" id="ScalarMult.__call__"><a class="viewcode-back" href="../../automod/pycalib.gp_classes.ScalarMult.html#pycalib.gp_classes.ScalarMult.__call__">[docs]</a>    <span class="nd">@params_as_tensors</span>
    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="c1"># Scalar multiplication</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span></div></div>


<span class="c1">############################</span>
<span class="c1">#    Models</span>
<span class="c1">############################</span>

<div class="viewcode-block" id="SVGPcal"><a class="viewcode-back" href="../../automod/pycalib.gp_classes.SVGPcal.html#pycalib.gp_classes.SVGPcal">[docs]</a><span class="k">class</span> <span class="nc">SVGPcal</span><span class="p">(</span><span class="n">gpflow</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">GPModel</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Probability calibration using a sparse variational latent Gaussian process.</span>

<span class="sd">    This is the Sparse Variational GP [1]_ calibration model. It has a single one-dimensional GP as a latent function</span>
<span class="sd">    which is applied to all inputs individually.</span>

<span class="sd">    .. [1] Hensman, J., Matthews, A. G. d. G. &amp; Ghahramani, Z. Scalable Variational Gaussian Process Classification in</span>
<span class="sd">           Proceedings of AISTATS (2015)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">kern</span><span class="p">,</span> <span class="n">likelihood</span><span class="p">,</span> <span class="n">feat</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">mean_function</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">num_latent</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">q_diag</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">whiten</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">minibatch_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">Z</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">num_data</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">q_mu</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">q_sqrt</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        - X is a data matrix, size N x D</span>
<span class="sd">        - Y is a data matrix, size N x P</span>
<span class="sd">        - kern, likelihood, mean_function are appropriate GPflow objects</span>
<span class="sd">        - Z is a matrix of pseudo inputs, size M x D</span>
<span class="sd">        - num_latent is the number of latent process to use, defaults to one.</span>
<span class="sd">        - q_diag is a boolean. If True, the covariance is approximated by a</span>
<span class="sd">          diagonal matrix.</span>
<span class="sd">        - whiten is a boolean. If True, we use the whitened representation of</span>
<span class="sd">          the inducing points.</span>
<span class="sd">        - minibatch_size, if not None, turns on mini-batching with that size.</span>
<span class="sd">        - num_data is the total number of observations, default to X.shape[0]</span>
<span class="sd">          (relevant when feeding in external minibatches)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># sort out the X, Y into MiniBatch objects if required.</span>
        <span class="k">if</span> <span class="n">minibatch_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">X</span> <span class="o">=</span> <span class="n">DataHolder</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="n">Y</span> <span class="o">=</span> <span class="n">DataHolder</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">X</span> <span class="o">=</span> <span class="n">Minibatch</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">minibatch_size</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">Y</span> <span class="o">=</span> <span class="n">Minibatch</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">minibatch_size</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="c1"># init the super class, accept args</span>
        <span class="k">if</span> <span class="n">num_latent</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">num_latent</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">GPModel</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">kern</span><span class="p">,</span> <span class="n">likelihood</span><span class="p">,</span> <span class="n">mean_function</span><span class="p">,</span> <span class="n">num_latent</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_data</span> <span class="o">=</span> <span class="n">num_data</span> <span class="ow">or</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q_diag</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">whiten</span> <span class="o">=</span> <span class="n">q_diag</span><span class="p">,</span> <span class="n">whiten</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">feature</span> <span class="o">=</span> <span class="n">features</span><span class="o">.</span><span class="n">inducingpoint_wrapper</span><span class="p">(</span><span class="n">feat</span><span class="p">,</span> <span class="n">Z</span><span class="p">)</span>

        <span class="c1"># init variational parameters</span>
        <span class="n">num_inducing</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">feature</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_init_variational_parameters</span><span class="p">(</span><span class="n">num_inducing</span><span class="p">,</span> <span class="n">q_mu</span><span class="p">,</span> <span class="n">q_sqrt</span><span class="p">,</span> <span class="n">q_diag</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_init_variational_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_inducing</span><span class="p">,</span> <span class="n">q_mu</span><span class="p">,</span> <span class="n">q_sqrt</span><span class="p">,</span> <span class="n">q_diag</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Constructs the mean and cholesky of the covariance of the variational Gaussian posterior.</span>
<span class="sd">        If a user passes values for `q_mu` and `q_sqrt` the routine checks if they have consistent</span>
<span class="sd">        and correct shapes. If a user does not specify any values for `q_mu` and `q_sqrt`, the routine</span>
<span class="sd">        initializes them, their shape depends on `num_inducing` and `q_diag`.</span>
<span class="sd">        Note: most often the comments refer to the number of observations (=output dimensions) with P,</span>
<span class="sd">        number of latent GPs with L, and number of inducing points M. Typically P equals L,</span>
<span class="sd">        but when certain multi-output kernels are used, this can change.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        :param num_inducing: int</span>
<span class="sd">            Number of inducing variables, typically referred to as M.</span>
<span class="sd">        :param q_mu: np.array or None</span>
<span class="sd">            Mean of the variational Gaussian posterior. If None the function will initialise</span>
<span class="sd">            the mean with zeros. If not None, the shape of `q_mu` is checked.</span>
<span class="sd">        :param q_sqrt: np.array or None</span>
<span class="sd">            Cholesky of the covariance of the variational Gaussian posterior.</span>
<span class="sd">            If None the function will initialise `q_sqrt` with identity matrix.</span>
<span class="sd">            If not None, the shape of `q_sqrt` is checked, depending on `q_diag`.</span>
<span class="sd">        :param q_diag: bool</span>
<span class="sd">            Used to check if `q_mu` and `q_sqrt` have the correct shape or to</span>
<span class="sd">            construct them with the correct shape. If `q_diag` is true,</span>
<span class="sd">            `q_sqrt` is two dimensional and only holds the square root of the</span>
<span class="sd">            covariance diagonal elements. If False, `q_sqrt` is three dimensional.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">q_mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">num_inducing</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_latent</span><span class="p">))</span> <span class="k">if</span> <span class="n">q_mu</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">q_mu</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q_mu</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">q_mu</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">settings</span><span class="o">.</span><span class="n">float_type</span><span class="p">)</span>  <span class="c1"># M x P</span>

        <span class="k">if</span> <span class="n">q_sqrt</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_diag</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">q_sqrt</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">num_inducing</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_latent</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">settings</span><span class="o">.</span><span class="n">float_type</span><span class="p">),</span>
                                        <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">positive</span><span class="p">)</span>  <span class="c1"># M x P</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">q_sqrt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">num_inducing</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">settings</span><span class="o">.</span><span class="n">float_type</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_latent</span><span class="p">)])</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">q_sqrt</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">q_sqrt</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">LowerTriangular</span><span class="p">(</span><span class="n">num_inducing</span><span class="p">,</span>
                                                                                     <span class="bp">self</span><span class="o">.</span><span class="n">num_latent</span><span class="p">))</span>  <span class="c1"># P x M x M</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">q_diag</span><span class="p">:</span>
                <span class="k">assert</span> <span class="n">q_sqrt</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">num_latent</span> <span class="o">=</span> <span class="n">q_sqrt</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">q_sqrt</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">q_sqrt</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">positive</span><span class="p">)</span>  <span class="c1"># M x L/P</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">assert</span> <span class="n">q_sqrt</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">num_latent</span> <span class="o">=</span> <span class="n">q_sqrt</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">num_inducing</span> <span class="o">=</span> <span class="n">q_sqrt</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">q_sqrt</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">q_sqrt</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">LowerTriangular</span><span class="p">(</span><span class="n">num_inducing</span><span class="p">,</span>
                                                                                     <span class="bp">self</span><span class="o">.</span><span class="n">num_latent</span><span class="p">))</span>  <span class="c1"># L/P x M x M</span>

<div class="viewcode-block" id="SVGPcal.build_prior_KL"><a class="viewcode-back" href="../../automod/pycalib.gp_classes.SVGPcal.html#pycalib.gp_classes.SVGPcal.build_prior_KL">[docs]</a>    <span class="nd">@params_as_tensors</span>
    <span class="k">def</span> <span class="nf">build_prior_KL</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">whiten</span><span class="p">:</span>
            <span class="n">K</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">K</span> <span class="o">=</span> <span class="n">Kuu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">feature</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kern</span><span class="p">,</span> <span class="n">jitter</span><span class="o">=</span><span class="n">settings</span><span class="o">.</span><span class="n">numerics</span><span class="o">.</span><span class="n">jitter_level</span><span class="p">)</span>  <span class="c1"># (P x) x M x M</span>

        <span class="k">return</span> <span class="n">kullback_leiblers</span><span class="o">.</span><span class="n">gauss_kl</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_mu</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_sqrt</span><span class="p">,</span> <span class="n">K</span><span class="p">)</span></div>

    <span class="nd">@params_as_tensors</span>
    <span class="k">def</span> <span class="nf">_build_likelihood</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This gives a variational bound on the model likelihood.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Get prior KL</span>
        <span class="n">KL</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">build_prior_KL</span><span class="p">()</span>

        <span class="c1"># Get conditionals</span>
        <span class="c1"># TODO: allow for block-diagonal covariance</span>
        <span class="n">fmeans</span><span class="p">,</span> <span class="n">fvars</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_predict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="n">full_cov</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">full_output_cov</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Get variational expectations</span>
        <span class="n">var_exp</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">likelihood</span><span class="o">.</span><span class="n">variational_expectations</span><span class="p">(</span><span class="n">fmeans</span><span class="p">,</span> <span class="n">fvars</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Y</span><span class="p">,</span> <span class="n">full_cov</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># re-scale for minibatch size</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_data</span><span class="p">,</span> <span class="n">settings</span><span class="o">.</span><span class="n">float_type</span><span class="p">)</span> <span class="o">/</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="n">settings</span><span class="o">.</span><span class="n">float_type</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">var_exp</span><span class="p">)</span> <span class="o">*</span> <span class="n">scale</span> <span class="o">-</span> <span class="n">KL</span>

    <span class="nd">@params_as_tensors</span>
    <span class="k">def</span> <span class="nf">_build_predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Xnew</span><span class="p">,</span> <span class="n">full_cov</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">full_output_cov</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute the mean and variance of :math:`p(f_* \\mid y)`.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        Xnew : np.array, shape=(N, K)</span>
<span class="sd">        full_cov : bool</span>
<span class="sd">        full_output_cov : bool</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        mus, vars :</span>
<span class="sd">            Mean and covariances of the variational approximation to the GP applied to the K input dimensions of Xnew.</span>
<span class="sd">            Dimensions: mus= N x K and vars= N x K (x K)</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Reshape to obtain correct covariance</span>
        <span class="n">num_data_new</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">Xnew</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">Xnew</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">Xnew</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

        <span class="c1"># Compute conditional</span>
        <span class="n">mu_tmp</span><span class="p">,</span> <span class="n">var_tmp</span> <span class="o">=</span> <span class="n">conditional</span><span class="p">(</span><span class="n">Xnew</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">feature</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kern</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_mu</span><span class="p">,</span> <span class="n">q_sqrt</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">q_sqrt</span><span class="p">,</span>
                                      <span class="n">full_cov</span><span class="o">=</span><span class="n">full_cov</span><span class="p">,</span>
                                      <span class="n">white</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">whiten</span><span class="p">,</span> <span class="n">full_output_cov</span><span class="o">=</span><span class="n">full_output_cov</span><span class="p">)</span>

        <span class="c1"># Reshape to N x K</span>
        <span class="n">mu</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">mu_tmp</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean_function</span><span class="p">(</span><span class="n">Xnew</span><span class="p">),</span> <span class="p">[</span><span class="n">num_data_new</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">])</span>
        <span class="n">var</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">var_tmp</span><span class="p">,</span> <span class="p">[</span><span class="n">num_data_new</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">])</span>

        <span class="k">return</span> <span class="n">mu</span><span class="p">,</span> <span class="n">var</span>

<div class="viewcode-block" id="SVGPcal.predict_f"><a class="viewcode-back" href="../../automod/pycalib.gp_classes.SVGPcal.html#pycalib.gp_classes.SVGPcal.predict_f">[docs]</a>    <span class="nd">@params_as_tensors</span>
    <span class="k">def</span> <span class="nf">predict_f</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_onedim</span><span class="p">,</span> <span class="n">full_cov</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">full_output_cov</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Predict the one-dimensional latent function</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X_onedim</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Compute conditional</span>
        <span class="n">mu</span><span class="p">,</span> <span class="n">var</span> <span class="o">=</span> <span class="n">conditional</span><span class="p">(</span><span class="n">X_onedim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">feature</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kern</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_mu</span><span class="p">,</span> <span class="n">q_sqrt</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">q_sqrt</span><span class="p">,</span>
                              <span class="n">full_cov</span><span class="o">=</span><span class="n">full_cov</span><span class="p">,</span>
                              <span class="n">white</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">whiten</span><span class="p">,</span> <span class="n">full_output_cov</span><span class="o">=</span><span class="n">full_output_cov</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">mu</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean_function</span><span class="p">(</span><span class="n">X_onedim</span><span class="p">),</span> <span class="n">var</span></div>

<div class="viewcode-block" id="SVGPcal.predict_full_density"><a class="viewcode-back" href="../../automod/pycalib.gp_classes.SVGPcal.html#pycalib.gp_classes.SVGPcal.predict_full_density">[docs]</a>    <span class="nd">@params_as_tensors</span>
    <span class="k">def</span> <span class="nf">predict_full_density</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Xnew</span><span class="p">):</span>
        <span class="n">pred_f_mean</span><span class="p">,</span> <span class="n">pred_f_var</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_predict</span><span class="p">(</span><span class="n">Xnew</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">likelihood</span><span class="o">.</span><span class="n">predict_full_density</span><span class="p">(</span><span class="n">pred_f_mean</span><span class="p">,</span> <span class="n">pred_f_var</span><span class="p">)</span></div></div>


<span class="c1">############################</span>
<span class="c1">#    Inverse link functions</span>
<span class="c1">############################</span>

<div class="viewcode-block" id="SoftArgMax"><a class="viewcode-back" href="../../automod/pycalib.gp_classes.SoftArgMax.html#pycalib.gp_classes.SoftArgMax">[docs]</a><span class="k">class</span> <span class="nc">SoftArgMax</span><span class="p">(</span><span class="n">Parameterized</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This class implements the multi-class softargmax inverse-link function. Given a vector :math:`f=[f_1, f_2, ... f_k]`,</span>
<span class="sd">    then result of the mapping is :math:`y = [y_1 ... y_k]`, where</span>
<span class="sd">    :math:`y_i = \\frac{\\exp(f_i)}{\\sum_{j=1}^k\\exp(f_j)}`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">=</span> <span class="n">num_classes</span>

<div class="viewcode-block" id="SoftArgMax.__call__"><a class="viewcode-back" href="../../automod/pycalib.gp_classes.SoftArgMax.html#pycalib.gp_classes.SoftArgMax.__call__">[docs]</a>    <span class="nd">@params_as_tensors</span>
    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">F</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">F</span><span class="p">)</span></div></div>


<span class="c1">############################</span>
<span class="c1">#    Likelihoods</span>
<span class="c1">############################</span>

<div class="viewcode-block" id="MultiCal"><a class="viewcode-back" href="../../automod/pycalib.gp_classes.MultiCal.html#pycalib.gp_classes.MultiCal">[docs]</a><span class="k">class</span> <span class="nc">MultiCal</span><span class="p">(</span><span class="n">gpflow</span><span class="o">.</span><span class="n">likelihoods</span><span class="o">.</span><span class="n">Likelihood</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span> <span class="n">invlink</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_monte_carlo_points</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        A likelihood that performs multiclass calibration using the softargmax link function and a single latent</span>
<span class="sd">        process.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        num_classes : int</span>
<span class="sd">            Number of classes.</span>
<span class="sd">        invlink : default=None</span>
<span class="sd">            Inverse link function :math:`p(y \mid f)`.</span>
<span class="sd">        num_monte_carlo_points : int, default=100</span>
<span class="sd">            Number of Monte-Carlo points for prediction, i.e. for the integral :math:`\int p(y=Y|f)q(f) df`, where</span>
<span class="sd">            :math:`q(f)` is a Gaussian.</span>
<span class="sd">        kwargs</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">=</span> <span class="n">num_classes</span>
        <span class="k">if</span> <span class="n">invlink</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">invlink</span> <span class="o">=</span> <span class="n">SoftArgMax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">)</span>
        <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">invlink</span><span class="p">,</span> <span class="n">SoftArgMax</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">invlink</span> <span class="o">=</span> <span class="n">invlink</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_monte_carlo_points</span> <span class="o">=</span> <span class="n">num_monte_carlo_points</span>

<div class="viewcode-block" id="MultiCal.logp"><a class="viewcode-back" href="../../automod/pycalib.gp_classes.MultiCal.html#pycalib.gp_classes.MultiCal.logp">[docs]</a>    <span class="k">def</span> <span class="nf">logp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">F</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Computes the log softargmax at indices from Y.</span>

<span class="sd">        :math:`\\sigma(F)_y = \\frac{exp(F_y)}{\\sum_{k=1}^K \\exp(F_k)}`</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        F : tf.tensor, shape=(N, K)</span>
<span class="sd">            Inputs to softargmax.</span>
<span class="sd">        Y : tf.tensor, shape=(N, 1)</span>
<span class="sd">            Indices of softargmax output.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        log_sigma_y : tf.tensor, shape=()</span>
<span class="sd">            log softargmax at y</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">invlink</span><span class="p">,</span> <span class="n">SoftArgMax</span><span class="p">):</span>
            <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">(</span>
                    <span class="p">[</span>
                        <span class="n">tf</span><span class="o">.</span><span class="n">assert_equal</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">Y</span><span class="p">)[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">),</span>
                        <span class="n">tf</span><span class="o">.</span><span class="n">assert_equal</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">F</span><span class="p">)[</span><span class="mi">1</span><span class="p">],</span> <span class="n">settings</span><span class="o">.</span><span class="n">int_type</span><span class="p">),</span>
                                        <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">settings</span><span class="o">.</span><span class="n">int_type</span><span class="p">))</span>
                    <span class="p">]):</span>
                <span class="k">return</span> <span class="o">-</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sparse_softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">F</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">Y</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])[:,</span> <span class="kc">None</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span></div>

<div class="viewcode-block" id="MultiCal.variational_expectations"><a class="viewcode-back" href="../../automod/pycalib.gp_classes.MultiCal.html#pycalib.gp_classes.MultiCal.variational_expectations">[docs]</a>    <span class="k">def</span> <span class="nf">variational_expectations</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Fmus</span><span class="p">,</span> <span class="n">Fvars</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">full_cov</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Computes an approximation to the expectation terms in the variational objective.</span>

<span class="sd">        This function approximates the :math:`n` expectation terms :math:`\\mathbb{E}_{q(f_n)}[\\log p(y_n \\mid f_n)]`</span>
<span class="sd">        in the variational objective function.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        Fmus : tf.tensor, shape=(N, K)</span>
<span class="sd">            Means of the latent GP at input locations X. Dimension N x K.</span>
<span class="sd">        Fvars : tf.tensor, shape=(N, K(, K))</span>
<span class="sd">            Variances of the latent GP at input locations X. Dimension N x K (x K).</span>
<span class="sd">        Y : tf.tensor, shape=(N,)</span>
<span class="sd">            Output vector.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        ve : tf.tensor, shape=(N,)</span>
<span class="sd">            The variational expectation assuming a Gaussian approximation q.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">invlink</span><span class="p">,</span> <span class="n">SoftArgMax</span><span class="p">):</span>
            <span class="c1"># Compute variational expectations by 2nd order Taylor approximation</span>
            <span class="n">sigma_mu</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">Fmus</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">full_cov</span><span class="p">:</span>
                <span class="n">sigSsig</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;nk, nkl, nl -&gt; n&quot;</span><span class="p">,</span> <span class="n">sigma_mu</span><span class="p">,</span> <span class="n">Fvars</span><span class="p">,</span> <span class="n">sigma_mu</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">sigSsig</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">sigma_mu</span><span class="p">,</span> <span class="n">Fvars</span><span class="p">),</span> <span class="n">sigma_mu</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

            <span class="n">diagSsig</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">sigma_mu</span><span class="p">,</span> <span class="n">Fvars</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">logsoftargmax_y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">logp</span><span class="p">(</span><span class="n">Fmus</span><span class="p">,</span> <span class="n">Y</span><span class="p">))</span>

            <span class="c1"># Objective function</span>
            <span class="k">return</span> <span class="n">logsoftargmax_y</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">sigSsig</span> <span class="o">-</span> <span class="n">diagSsig</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span></div>

<div class="viewcode-block" id="MultiCal.predict_mean_and_var"><a class="viewcode-back" href="../../automod/pycalib.gp_classes.MultiCal.html#pycalib.gp_classes.MultiCal.predict_mean_and_var">[docs]</a>    <span class="k">def</span> <span class="nf">predict_mean_and_var</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Fmus</span><span class="p">,</span> <span class="n">Fvars</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Given a Normal distribution for the latent function, return the mean of :math:`Y`, if</span>
<span class="sd">        :math:`q(f) = N(Fmu, Fvar)` and this object represents :math:`p(y|f)`, then this method computes the predictive</span>
<span class="sd">        mean :math:`\\int\\int y p(y|f)q(f) df dy` and the predictive variance</span>
<span class="sd">        :math:`\\int\\int y^2 p(y|f)q(f) df dy  - [ \\int\\int y p(y|f)q(f) df dy ]^2`.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        Fmus : array/tensor, shape=(N, D)</span>
<span class="sd">            Mean(s) of Gaussian density.</span>
<span class="sd">        Fvars : array/tensor, shape=(N, D(, D))</span>
<span class="sd">            Covariance(s) of Gaussian density.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span></div>

<div class="viewcode-block" id="MultiCal.predict_density"><a class="viewcode-back" href="../../automod/pycalib.gp_classes.MultiCal.html#pycalib.gp_classes.MultiCal.predict_density">[docs]</a>    <span class="k">def</span> <span class="nf">predict_density</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Fmus</span><span class="p">,</span> <span class="n">Fvars</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Given a Normal distribution for the latent function, and a datum Y, compute the log predictive density of Y.</span>
<span class="sd">        i.e. if :math:`p(f_* | y) = \\mathcal{N}(Fmu, Fvar)` and :math:`p(y_*|f_*)` is the likelihood, then this</span>
<span class="sd">        method computes the log predictive density :math:`\\log \\int p(y_*|f)p(f_* | y) df`. Here, we implement a</span>
<span class="sd">        Monte-Carlo routine.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        Fmus : array/tensor, shape=(N, K)</span>
<span class="sd">            Mean(s) of Gaussian density.</span>
<span class="sd">        Fvars : array/tensor, shape=(N, K(, K))</span>
<span class="sd">            Covariance(s) of Gaussian density.</span>
<span class="sd">        Y : arrays/tensors, shape=(N(, K))</span>
<span class="sd">            Deterministic arguments to be passed by name to funcs.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        log_density : array/tensor, shape=(N(, K))</span>
<span class="sd">            Log predictive density.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">invlink</span><span class="p">,</span> <span class="n">SoftArgMax</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">ndiag_mc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">logp</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_monte_carlo_points</span><span class="p">,</span> <span class="n">Fmus</span><span class="p">,</span> <span class="n">Fvars</span><span class="p">,</span>
                            <span class="n">logspace</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">Y</span><span class="o">=</span><span class="n">Y</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span></div>

<div class="viewcode-block" id="MultiCal.predict_full_density"><a class="viewcode-back" href="../../automod/pycalib.gp_classes.MultiCal.html#pycalib.gp_classes.MultiCal.predict_full_density">[docs]</a>    <span class="k">def</span> <span class="nf">predict_full_density</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Fmus</span><span class="p">,</span> <span class="n">Fvars</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">invlink</span><span class="p">,</span> <span class="n">SoftArgMax</span><span class="p">):</span>
            <span class="c1"># Sample from standard normal</span>
            <span class="n">N</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">Fmus</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">epsilon</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">num_monte_carlo_points</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">),</span>
                                       <span class="n">dtype</span><span class="o">=</span><span class="n">settings</span><span class="o">.</span><span class="n">float_type</span><span class="p">)</span>

            <span class="c1"># Transform to correct mean and covariance</span>
            <span class="n">f_star</span> <span class="o">=</span> <span class="n">Fmus</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">Fvars</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:])</span> <span class="o">*</span> <span class="n">epsilon</span>  <span class="c1"># S x N x K</span>

            <span class="c1"># Compute Softmax</span>
            <span class="n">p_y_f_star</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">f_star</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

            <span class="c1"># Average to obtain log Monte-Carlo estimate</span>
            <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">p_y_f_star</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span></div></div>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, Jonathan Wenger

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>